# MCP Code API - Multi-Provider Code Generation Server

[![CI](https://github.com/cecil-the-coder/mcp-code-api/actions/workflows/ci.yml/badge.svg)](https://github.com/cecil-the-coder/mcp-code-api/actions/workflows/ci.yml)
[![Release](https://github.com/cecil-the-coder/mcp-code-api/actions/workflows/release.yml/badge.svg)](https://github.com/cecil-the-coder/mcp-code-api/actions/workflows/release.yml)
[![Go Version](https://img.shields.io/badge/Go-1.21+-blue.svg)](https://golang.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![GitHub release](https://img.shields.io/github/v/release/cecil-the-coder/mcp-code-api)](https://github.com/cecil-the-coder/mcp-code-api/releases/latest)

A high-performance **Model Context Protocol (MCP) server** supporting multiple AI providers (Cerebras, OpenRouter, OpenAI, Anthropic, Gemini, and more). Designed for **planning with Claude Code, Cline, or Cursor** while leveraging fast providers like Cerebras for code generation to maximize speed and avoid API limits.

## ğŸš€ Why Go?

The Go implementation offers significant advantages over the Node.js version:
- **10x faster performance** for large code generation tasks
- **Single binary deployment** - no Node.js runtime required
- **Lower memory footprint** and better resource utilization
- **Cross-platform compilation** for easy deployment
- **Type safety** and better error handling
- **Concurrent processing** for handling multiple requests

## âœ¨ Features

- ğŸ¯ **Smart API Routing** with automatic fallback between Cerebras and OpenRouter
- ğŸ”§ **Single 'write' Tool** for ALL code operations (creation, editing, generation)
- ğŸ¨ **Enhanced Visual Diffs** with emoji indicators (âœ… additions, âŒ removals, ğŸ” changes)
- ğŸ”„ **Auto-Instruction System** that enforces proper MCP tool usage
- ğŸ“ **Context-Aware Processing** with multiple file support
- ğŸ’» **Multi-IDE Support** - Claude Code, Cursor, Cline, VS Code
- âš™ï¸ **Interactive Configuration Wizard** for easy setup
- ğŸ“ **Comprehensive Logging** with debug support

## ğŸ“‹ System Requirements

- **Go 1.21+** (for building from source)
- **Cerebras API Key** (primary) or **OpenRouter API Key** (fallback)
- **Supported IDE**: Claude Code, Cursor, Cline, or VS Code

## ğŸš€ Quick Start

### Option 1: Install from Binary (Recommended)

```bash
# Download the latest release for your platform
curl -L https://github.com/cecil-the-coder/mcp-code-api/releases/latest/download/mcp-code-api-$(uname -s)-$(uname -m) -o mcp-code-api

# Make it executable
chmod +x mcp-code-api

# Move to your PATH
sudo mv mcp-code-api /usr/local/bin/
```

### Option 2: Build from Source

```bash
# Clone the repository
git clone https://github.com/cecil-the-coder/mcp-code-api.git
cd mcp-code-api

# Build the binary
make build

# Install to system PATH
make install
```

## ğŸ“± Configuration

### 1. Run the Configuration Wizard

```bash
mcp-code-api config
```

The wizard will guide you through:
- Setting up API keys for Cerebras and/or OpenRouter
- Configuring your preferred IDE
- Testing API connections
- Generating configuration files

### 2. Set API Keys (Optional Manual Setup)

```bash
# Cerebras API (Primary)
export CEREBRAS_API_KEY="your_cerebras_api_key"

# OpenRouter API (Optional Fallback)
export OPENROUTER_API_KEY="your_openrouter_api_key"

# Set model preferences (optional)
export CEREBRAS_MODEL="zai-glm-4.6"
export OPENROUTER_MODEL="qwen/qwen3-coder"
```

### 3. Start the MCP Server

```bash
mcp-code-api server
```

## ğŸ’» IDE Integration

### Claude Code

The configuration wizard automatically sets up Claude Code. After configuration:

1. Restart Claude Code
2. The `write` tool will appear in your tool list
3. Use it for all code operations

### Cursor

1. Run the configuration wizard
2. Copy the generated rules to Cursor â†’ Settings â†’ Developer â†’ User Rules
3. Restart Cursor

### Cline

1. Run the configuration wizard
2. Restart Cline
3. The `write` tool will be available

### VS Code

1. Install an MCP extension for VS Code
2. Run the configuration wizard
3. Restart VS Code
4. The `write` tool will be available via MCP

## ğŸ”§ Usage

The MCP tool provides a single `write` tool that handles ALL code operations:

### Basic Usage

```bash
# In your IDE, use natural language:

"Create a REST API with Express.js that handles user authentication"

"Add input validation to the login function in auth.js"

"Generate a Python script that processes CSV files and outputs to JSON"
```

### Advanced Usage with Context Files

```bash
"Refactor the database connection in models.js using the pattern from utils.js"

# The tool will automatically read context files:
# - models.js (existing file to modify)
# - utils.js (context for patterns)
```

### Parameters

The `write` tool accepts:

- **file_path** (required): Absolute path to the target file
- **prompt** (required): Detailed description of what to create/modify
- **context_files** (optional): Array of file paths for context

## ğŸ¨ Visual Diffs

The Go implementation enhances visual diffs with:

- âœ… **Green indicators** for new lines
- âŒ **Red indicators** for removed lines
- ğŸ” **Change indicators** for modified content
- ğŸ“Š **Summary statistics** (additions, removals, modifications)
- ğŸ“ **Full file paths** for clarity

## ğŸ”’ Auto-Instruction System

The Go implementation includes an enhanced auto-instruction system that:

- Automatically enforces MCP tool usage
- Prevents direct file editing
- Provides clear instructions to AI models
- Ensures consistent behavior across all IDEs

## ğŸ—ï¸ Development

### Building

```bash
# Build for current platform
make build

# Build for Linux (cross-compile)
make linux

# Build all platforms
make release
```

### Testing

```bash
# Run tests
make test

# Run tests with coverage
make coverage
```

### Code Quality

```bash
# Format code
make format

# Run linter
make lint
```

### Docker

```bash
# Build Docker image
make docker-build

# Run Docker container
make docker-run
```

## ğŸ“ Project Structure

```
mcp-code-api/
â”œâ”€â”€ ğŸ“„ go.mod              # Go module definition
â”œâ”€â”€ ğŸ“„ main.go              # Entry point
â”œâ”€â”€ ğŸ“ cmd/                 # CLI commands
â”‚   â”œâ”€â”€ ğŸ“œ root.go          # Root command
â”‚   â”œâ”€â”€ ğŸ“œ server.go        # Server command
â”‚   â””â”€â”€ ğŸ“œ config.go        # Configuration command
â”œâ”€â”€ ğŸ“ internal/            # Internal packages
â”‚   â”œâ”€â”€ ğŸ“ api/             # API integrations
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ router.go    # API router
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ cerebras.go  # Cerebras client
â”‚   â”‚   â””â”€â”€ ğŸ“œ openrouter.go # OpenRouter client
â”‚   â”œâ”€â”€ ğŸ“ config/          # Configuration management
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ config.go     # Configuration types
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ constants.go # Constants
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ utils.go      # Utility functions
â”‚   â”‚   â””â”€â”€ ğŸ“ interactive/ # Interactive wizards
â”‚   â”œâ”€â”€ ğŸ“ mcp/             # MCP server implementation
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ server.go    # Main MCP server
â”‚   â”‚   â””â”€â”€ ğŸ“œ write_tool.go # Write tool handler
â”‚   â”œâ”€â”€ ğŸ“ utils/           # General utilities
â”‚   â”‚   â””â”€â”€ ğŸ“œ file_utils.go # File operations
â”‚   â”œâ”€â”€ ğŸ“ formatting/      # Response formatting
â”‚   â”‚   â””â”€â”€ ğŸ“œ response_formatter.go # Visual diffs
â”‚   â””â”€â”€ ğŸ“ logger/          # Logging system
â”‚       â””â”€â”€ ğŸ“œ logger.go      # Logger implementation
â”œâ”€â”€ ğŸ“„ Makefile             # Build automation
â”œâ”€â”€ ğŸ“„ README.md            # This file
â””â”€â”€ ğŸ“„ LICENSE              # MIT License
```

## ğŸ”§ Configuration Options

### Environment Variables

```bash
# Cerebras Configuration
CEREBRAS_API_KEY=your_key
CEREBRAS_MODEL=zai-glm-4.6
CEREBRAS_TEMPERATURE=0.6
CEREBRAS_MAX_TOKENS=4096

# OpenRouter Configuration
OPENROUTER_API_KEY=your_key
OPENROUTER_MODEL=qwen/qwen3-coder
OPENROUTER_SITE_URL=https://github.com/your-repo
OPENROUTER_SITE_NAME=Your Project

# Server Configuration
CEREBRAS_MCP_LOG_LEVEL=info
CEREBRAS_MCP_LOG_FILE=/path/to/logfile
CEREBRAS_MCP_DEBUG=false
CEREBRAS_MCP_VERBOSE=false
```

### Configuration File

You can also use a YAML configuration file at `~/.mcp-code-api/config.yaml`:

```yaml
cerebras:
  api_key: "your_key"
  model: "zai-glm-4.6"
  temperature: 0.6
  max_tokens: 4096

openrouter:
  api_key: "your_key"
  model: "qwen/qwen3-coder"
  site_url: "https://github.com/your-repo"
  site_name: "Your Project"

logging:
  level: "info"
  verbose: false
  debug: false
  file: "/path/to/logfile"
```

### Load Balancing & Failover

The server supports multiple API keys per provider for automatic load balancing and failover:

#### Multiple API Keys Configuration

```yaml
providers:
  cerebras:
    # Multiple keys - automatically load balanced
    api_keys:
      - "${CEREBRAS_API_KEY_1}"
      - "${CEREBRAS_API_KEY_2}"
      - "${CEREBRAS_API_KEY_3}"
    model: "zai-glm-4.6"

  openrouter:
    # Single key - backward compatible
    api_key: "${OPENROUTER_API_KEY}"
    model: "qwen/qwen3-coder"
```

#### How It Works

- **Round-robin load balancing**: Requests are evenly distributed across all configured keys
- **Automatic failover**: If one key fails (rate limit, error), automatically tries the next available key
- **Exponential backoff**: Failed keys enter backoff period: 1s â†’ 2s â†’ 4s â†’ 8s â†’ max 60s
- **Health tracking**: System monitors each key's health and skips unhealthy keys
- **Auto-recovery**: Keys automatically recover and rejoin rotation after backoff period

#### Benefits

- **Rate limit avoidance**: Multiply your effective rate limit by using multiple keys
- **High availability**: Service continues even if some keys fail or are rate limited
- **Better throughput**: Distribute load across multiple keys for higher concurrency
- **Fault tolerance**: Automatic recovery from transient failures

#### Recommended Setup

- **Light usage**: 1 key is sufficient
- **Production**: 2-3 keys recommended for failover capability
- **High volume**: 3-5 keys for optimal performance and resilience

#### Example with Environment Variables

```bash
# Set multiple keys
export CEREBRAS_API_KEY_1="csk-primary-xxxxx"
export CEREBRAS_API_KEY_2="csk-secondary-xxxxx"
export CEREBRAS_API_KEY_3="csk-tertiary-xxxxx"

# Start server - will automatically use all configured keys
mcp-code-api server
```

For a complete example configuration, see [config.example.yaml](config.example.yaml).

## ğŸ”Œ Using API-Compatible Providers

The server supports **API-compatible providers** - third-party services that implement the same API format as the major providers. This includes:

- **Anthropic-compatible** (e.g., z.ai with GLM-4.6, local proxies)
- **OpenAI-compatible** (e.g., LM Studio, Ollama, LocalAI)
- **Custom self-hosted endpoints**

### Anthropic-Compatible Providers (z.ai)

The MCP Code API supports any provider that implements the Anthropic Messages API format.

#### Configuration File Method

Add to your `~/.mcp-code-api/config.yaml`:

```yaml
providers:
  anthropic:
    # z.ai's authentication token
    api_key: "your-zai-api-key"
    # z.ai's Anthropic-compatible endpoint
    base_url: "https://api.z.ai/api/anthropic"
    # Use Z.ai's GLM-4.6 model (200K context, optimized for coding)
    model: "glm-4.6"

  enabled:
    - anthropic

  preferred_order:
    - anthropic
```

**Available Z.ai Models:**
- `glm-4.6` - Latest flagship model (200K context, best for coding/reasoning)
- `glm-4.5-air` - Lighter/faster variant for quick tasks

#### Environment Variables Method

```bash
# z.ai example
export ANTHROPIC_AUTH_TOKEN="your-zai-api-key"
export ANTHROPIC_BASE_URL="https://api.z.ai/api/anthropic"

# Start the server
./mcp-code-api server
```

**Note**: Both `ANTHROPIC_API_KEY` and `ANTHROPIC_AUTH_TOKEN` environment variables are supported.

#### Multiple Anthropic Providers (Advanced)

If you want to use both standard Anthropic AND a compatible provider:

```yaml
providers:
  # Standard Anthropic
  anthropic:
    api_key: "sk-ant-..."
    base_url: "https://api.anthropic.com"
    model: "claude-3-5-sonnet-20241022"

  # Custom provider: z.ai
  custom:
    zai:
      type: "anthropic"
      name: "Z.ai"
      api_key: "your-zai-api-key"
      base_url: "https://api.z.ai/api/anthropic"
      default_model: "glm-4.6"
      supports_streaming: true
      supports_tool_calling: true
      tool_format: "anthropic"

  enabled:
    - anthropic
    - zai

  preferred_order:
    - zai         # Try z.ai first
    - anthropic   # Fall back to official Anthropic
```

### OpenAI-Compatible Providers (LM Studio, Ollama)

```yaml
providers:
  openai:
    api_key: "lm-studio"  # Can be any value for LM Studio
    base_url: "http://localhost:1234/v1"
    model: "local-model"
```

Or using environment variables:

```bash
export OPENAI_API_KEY="lm-studio"
export OPENAI_BASE_URL="http://localhost:1234/v1"
```

### Supported Environment Variables

All providers now support custom base URLs via environment variables:

| Provider   | API Key Env Var(s)                    | Base URL Env Var       |
|-----------|---------------------------------------|------------------------|
| Anthropic | `ANTHROPIC_API_KEY`, `ANTHROPIC_AUTH_TOKEN` | `ANTHROPIC_BASE_URL` |
| OpenAI    | `OPENAI_API_KEY`                      | `OPENAI_BASE_URL`     |
| Gemini    | `GEMINI_API_KEY`                      | `GEMINI_BASE_URL`     |
| Qwen      | `QWEN_API_KEY`                        | `QWEN_BASE_URL`       |
| Cerebras  | `CEREBRAS_API_KEY`                    | `CEREBRAS_BASE_URL`   |
| OpenRouter| `OPENROUTER_API_KEY`                  | `OPENROUTER_BASE_URL` |

**Examples:**

```bash
# Use an OpenAI-compatible endpoint (like LM Studio)
export OPENAI_API_KEY="lm-studio-key"
export OPENAI_BASE_URL="http://localhost:1234/v1"

# Use a custom Anthropic-compatible endpoint (z.ai)
export ANTHROPIC_AUTH_TOKEN="your-token"
export ANTHROPIC_BASE_URL="https://api.z.ai/api/anthropic"
```

### Troubleshooting

**Authentication fails:**
- Verify your token/API key is correct
- Check if the base URL includes the correct API version path
- Some providers require specific headers - check their documentation

**Different API format:**
If the provider uses a slightly different format, you may need to create a custom provider adapter.

**Rate limiting:**
Some compatible providers have different rate limits than the official APIs. Adjust your usage accordingly.

## ğŸ¤ Contributing

Contributions are welcome! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- ğŸ“§ **Email**: support@cerebras.ai
- ğŸ› **Issues**: [GitHub Issues](https://github.com/cecil-the-coder/mcp-code-api/issues)
- ğŸ“š **Documentation**: [Wiki](https://github.com/cecil-the-coder/mcp-code-api/wiki)
- ğŸ’¬ **Community**: [Discussions](https://github.com/cecil-the-coder/mcp-code-api/discussions)

## ğŸ”— Related Projects

- [Cerebras Node.js MCP Server](https://github.com/cerebras/cerebras-mcp) - Original Node.js implementation
- [Cerebras AI Platform](https://cloud.cerebras.ai) - AI platform
- [Model Context Protocol](https://modelcontextprotocol.io) - MCP specification

## ğŸ¯ Roadmap

- [ ] **Real-time streaming** for large code generation
- [ ] **Plugin system** for custom tools
- [ ] **Workspace management** for project-level operations
- [ ] **Performance monitoring** and metrics
- [ ] **Advanced caching** for faster responses
- [ ] **Multi-model support** with automatic selection

---

**âš¡ Built with Go for maximum performance and reliability**